---
title: "task1_baseball_dataset"
author: "Adam"
date: "2024-09-18"
output: html_document
---

Installing libraries and dependencies for this R Markdown notebook

```{r}
if (!require(tidyverse)) install.packages('tidyverse', dependencies=TRUE)
if (!require(corrplot)) install.packages('corrplot', dependencies=TRUE)
if (!require(rmarkdown)) install.packages('rmarkdown', dependencies=TRUE)
if (!require(gridextra)) install.packages('gridExtra', dependencies=TRUE)

if (!require(tidyverse)) install.packages('tidyverse', dependencies=TRUE)
if (!require(rpart)) install.packages('rpart', dependencies=TRUE)
if (!require(rpart.plot)) install.packages('rpart.plot', dependencies=TRUE)
if (!require(caret)) install.packages('caret', dependencies=TRUE)

library(tidyverse)
library(corrplot)
library(rmarkdown)
library(gridExtra)

library(tidyverse)
library(rpart)
library(rpart.plot)
library(caret)
```

load the database file

```{r}
library(readr)
data <- read_csv("baseball_dataset.csv")
View(baseball_dataset)
```

------------------------------------------------------------------------

### Step 1: Covariance and Correlation Analysis

```{r}
# Function to compute covariance and correlation
compute_cov_cor <- function(data) {
  data_filtered <- select(data, R, E, HR, RA, SOA) # Selecting relevant columns
  cov_matrix <- cov(data_filtered) # Covariance matrix
  cor_matrix <- cor(data_filtered) # Correlation matrix
  list(cov_matrix = cov_matrix, cor_matrix = cor_matrix)
}

# Call the function to compute covariance and correlation
results <- compute_cov_cor(data)

cat("Covariance Matrix:\n")
print(results$cov_matrix)

cat("\nCorrelation Matrix:\n")
print(results$cor_matrix)

corrplot(results$cor_matrix, method = "color", title = "Correlation Matrix", mar=c(0,0,1,0))
```

### Interpretation:

(click on the icons to flip between the two outputs, covariance and correlation)\
Covariance measures how much two variables change together. If the covariance is positive, they increase together.\
A large positive covariance indicates that the two variables are positively correlated, while a large negative covariance indicates negative correlation.

-   **R (Runs Scored) and HR (Home Runs)**: The covariance between runs scored and home runs is 4449.84, which is positive and relatively large, indicating that more home runs tend to result in more runs scored.
-   **E (Errors) and RA (Opponents Runs Scored)**: The covariance between errors and opponents' runs scored is -89.67, a very small negative value, suggesting no strong relationship between these two.
-   **E (Errors) and SOA (Strikeouts by Pitchers)**: The negative covariance of -23019.08 suggests that more errors are associated with fewer strikeouts by pitchers, which makes sense because poor fielding (more errors) might indicate weaker overall team defense.

Correlation values range between -1 and 1. A value closer to 1 shows a strong positive relationship, whereas a value close to -1 indicates a strong negative relationship.

-   **R (Runs Scored) and HR (Home Runs)**: The correlation is 0.50, showing a moderate positive relationship. As expected, teams that hit more home runs also tend to score more runs.
-   **E (Errors) and HR (Home Runs)**: The correlation is -0.69, which indicates a strong negative relationship. This suggests that teams making more errors tend to hit fewer home runs.
-   **HR (Home Runs) and SOA (Strikeouts by Pitchers)**: A correlation of 0.80 suggests a strong positive relationship. This might indicate that teams with more home runs tend to have pitchers who get more strikeouts, which could be reflective of overall team quality.

------------------------------------------------------------------------

## Step 2: Scatter Plot Analysis

### Scatter Plot for AB (At Bats) vs H (Hits by Batters) and HA (Hits Allowed) vs BBA (Walks Allowed)

```{r}
create_scatter_plots <- function(data) {
  
  # Scatter plot, AB vs H
  p1 <- ggplot(data, aes(x=AB, y=H)) +
    geom_point(color="blue") +
    labs(title="At Bats vs Hits", x="At Bats (AB)", y="Hits (H)") +
    theme_minimal()
  
  # Scatter plot, HA vs BBA
  p2 <- ggplot(data, aes(x=HA, y=BBA)) +
    geom_point(color="green") +
    labs(title="Hits Allowed vs Walks Allowed", x="Hits Allowed (HA)", y="Walks Allowed (BBA)") +
    theme_minimal()
  
  gridExtra::grid.arrange(p1, p2, ncol = 2)
}

create_scatter_plots(data)
```

### Interpretation:

placeholder text

------------------------------------------------------------------------

## Step 3: Histograms for Teams Across Two 10-Year Periods

### Selecting Teams

Here, we randomly select two teams in addition to the Houston Astros and visualize the distribution of `TARGET` (High, Average, Low) for the two periods: 2004-2013 and 2014-2023.

I used a set `randomness seed (31)` in this code so that the analysis section would still line up with the random teams when this code is run multiple times. Changing the randomness seed will change the chosen teams.

```{r}
# Filter the dataset for years after 2004
data_after_2004 <- data %>% filter(yearID >= 2004)

# Randomly select two teams, excluding the Houston Astros
teams_list <- unique(data_after_2004$name)
teams_list <- teams_list[teams_list != "Houston Astros"]

set.seed(30)  # seed for reproducibility

random_teams <- sample(teams_list, 2)
random_teams <- c("Houston Astros", random_teams) 

create_histograms <- function(team_name, data) {
  data_2004_2013 <- data %>% filter(name == team_name & yearID >= 2004 & yearID <= 2013)
  data_2014_2023 <- data %>% filter(name == team_name & yearID >= 2014 & yearID <= 2023)

data_2004_2013 <- data_2004_2013 %>%
  mutate(TARGET = factor(TARGET, levels = c("LOW", "AVERAGE", "HIGH")))

data_2014_2023 <- data_2014_2023 %>%
  mutate(TARGET = factor(TARGET, levels = c("LOW", "AVERAGE", "HIGH")))

# Create histograms for TARGET class distribution for both time periods
p1 <- ggplot(data_2004_2013, aes(x=TARGET, fill=TARGET)) +
  geom_bar() +
  labs(title=paste(team_name, "2004-2013"), x="TARGET Class", y="Count") +
  scale_x_discrete(drop=FALSE) +  
  theme_minimal()+
  theme(legend.position = "none")

p2 <- ggplot(data_2014_2023, aes(x=TARGET, fill=TARGET)) +
  geom_bar() +
  labs(title=paste(team_name, "2014-2023"), x="TARGET Class", y="Count") +
  scale_x_discrete(drop=FALSE) + 
  theme_minimal()+
  theme(legend.position = "none")

  gridExtra::grid.arrange(p1, p2, ncol = 2)
}

for (team in random_teams) {
  print(paste("Creating histograms for team:", team))
  create_histograms(team, data_after_2004)
}

```

### Interpretation:

1.  Houston Astros (2004-2013 and 2014-2023)

2.  Oakland Athletics (2004-2013 and 2014-2023)

3.  Texas Rangers (2004-2013 and 2014-2023)

------------------------------------------------------------------------

## Step 4: BB & SB Box Plots

```{r}
# Filter out data that has the TARGET column (High/Average/Low)
data_filtered <- data %>% filter(!is.na(TARGET))

# create box plots for a specific attribute and compare across TARGET classes
create_boxplots <- function(data, attribute) {
  
  p1 <- ggplot(data, aes(x=TARGET, y=get(attribute), fill=TARGET)) +
    geom_boxplot() +
    labs(title=paste(attribute, "by TARGET Class"), x="TARGET Class", y=attribute) +
    theme_minimal()
  
  p2 <- ggplot(data, aes(x="All Instances", y=get(attribute))) +
    geom_boxplot(fill="lightblue") +
    labs(title=paste(attribute, "for All Instances"), x="All Instances", y=attribute) +
    theme_minimal()

  gridExtra::grid.arrange(p1, p2, ncol = 2)
}

# Create box plots for BB (Walks by Batters)
create_boxplots(data_filtered, "BB")

# Create box plots for SB (Stolen Bases)
create_boxplots(data_filtered, "SB")
```

### Interpretation:

BB (Walks by Batters):

------------------------------------------------------------------------

## Step 5: supervised scatter plots

```{r}
# Filter out data that has the TARGET column (High/Average/Low)
data_filtered <- data %>% filter(!is.na(TARGET))

create_supervised_scatterplot <- function(data, x_attr, y_attr) {
  
  plot <- ggplot(data, aes_string(x=x_attr, y=y_attr, color="TARGET")) +
    geom_point(alpha=0.7, size=3) +
    labs(title=paste("Supervised Scatter Plot:", x_attr, "vs", y_attr),
         x=x_attr, y=y_attr) +
    theme_minimal() +
    scale_color_manual(values = c("LOW" = "red", "AVERAGE" = "yellow", "HIGH" = "green")) +
    theme(legend.position = "bottom")
  
  print(plot)
}

# Scatter plots for the pairs HB/SO, CG/SHO, and IPOuts/DP
create_supervised_scatterplot(data_filtered, "HBP", "SO")
create_supervised_scatterplot(data_filtered, "CG", "SHO")
create_supervised_scatterplot(data_filtered, "IPouts", "DP")
```

### Interpretation:

In the HBP vs SO plot, most of the datapoints concentrate at the average low HBP which is 40 and a wide range of SO which is around 250 to 1500. 120-160 HBP could be interpreted as outliers. There is no recognizable correlation between the strike outs and hit by pitcher across all of the performance level, but it looks like team with “high” target falls in the higher numbers of hit by pitcher.

In the CG vs SHO plot, there are datapoints across all values of CG and they mostly concentrate at less than 100. But there is a trend in the SHO values. It is divided into 3 layers with teams in the “low” target are mostly at bottom, teams in the “average” target stay at the middle and teams in the “high” target stay at the top. It represents that the SHO could be used to predict the win percentage. 

The IPouts vs DP plot has a small correlation here. The higher number of IPouts means higher number of DPs. Some records of “low” target teams lies in the corner near 0 but the the majority of the data falls between 3000-4500IPouts and 50-200 DP.

------------------------------------------------------------------------

## Step 6: comparing density plots

```{r}
# Filter out data that has the TARGET column (High/Average/Low)
data_filtered <- data %>% filter(!is.na(TARGET))

# Compute Wins Percentage (W / G) and Errors Per Game (E / G)
data_filtered <- data_filtered %>%
  mutate(WP = W / G, EPG = E / G)

# density plots for Wins Percentage and Errors Per Game
create_density_plots <- function(data) {
  
  p1 <- ggplot(data, aes(x=WP, fill=TARGET)) +
    geom_density(alpha=0.5) +
    labs(title="Density Plot: Wins Percentage by TARGET", x="Wins Percentage", y="Density") +
    theme_minimal() +
    scale_fill_manual(values = c("LOW" = "red", "AVERAGE" = "yellow", "HIGH" = "green")) +
    theme(legend.position = "bottom")
  
  p2 <- ggplot(data, aes(x=EPG, fill=TARGET)) +
    geom_density(alpha=0.5) +
    labs(title="Density Plot: Errors Per Game by TARGET", x="Errors Per Game", y="Density") +
    theme_minimal() +
    scale_fill_manual(values = c("LOW" = "red", "AVERAGE" = "yellow", "HIGH" = "green")) +
    theme(legend.position = "bottom")

  gridExtra::grid.arrange(p1, p2, ncol = 2)
  
}

create_density_plots(data_filtered)
```

### Interpretation:

Generally, the distribution of all 3 classes of the TARGET attribute are expected. Teams in the “Low” class has the wins percentage in the range of around 0 to 0.4 with a peak at 0.375. Teams in the “Average” class falls in the range of 0.375 to 0.64 and the win percentage of “High” class teams is higher than 0.6 with a peak at 0.64.

In the density plot of errors per game by Target, the peak of the “High” teams is the nearest to 0 indicating that they usually make very small amount of errors. “Low” teams has a spread of errors per game between 1 to 1.5 indicating their inconsistency in performance. Teams in “average” class has a very high peak between the “high” and “low” representing their moderate errors at approximately 0.75  in each game.

To sum up, teams with high wins percentage make less errors per game and their performance is more consistent. In contrast, teams with less wins percentage make more errors and have high variability in their performance.

------------------------------------------------------------------------

## Step 7: supervised scatter plots

```{r}
# Filter data for teams that won the World Series (WSWin = 'Y')
ws_winners <- data %>% filter(WSWin == "Y")

# Create table that counts number of each TARGET class
team_target_counts <- ws_winners %>%
  group_by(name) %>%
  summarise(
    High_Count = sum(TARGET == "HIGH"),
    Average_Count = sum(TARGET == "AVERAGE"),
    Low_Count = sum(TARGET == "LOW"),
    Total_WS_Wins = n()  # Count of how many times the team won the WS
  )
print(team_target_counts)

# histograms for (W) and (L) for each team
create_histograms_for_team <- function(team_name, data) {
  # Filter data for selected team
  team_data <- data %>% filter(name == team_name)

  # (W)
  p1 <- ggplot(team_data, aes(x=W)) +
    geom_histogram(binwidth = 2, fill="blue", color="black") +
    labs(title=paste(team_name, "- Wins"), x="Wins", y="Count") +
    theme_minimal()

  # (L)
  p2 <- ggplot(team_data, aes(x=L)) +
    geom_histogram(binwidth = 2, fill="red", color="black") +
    labs(title=paste(team_name, "- Losses"), x="Losses", y="Count") +
    theme_minimal()

  gridExtra::grid.arrange(p1, p2, ncol = 2)
}

# Loop, every team that won WorldSeries, create histograms for W/L
teams <- unique(ws_winners$name)
for (team in teams) {
  print(paste("Creating histograms for team:", team))
  create_histograms_for_team(team, ws_winners)
}
```

### Interpretation:

The table illustrates the  performance of teams that won the World Series. The value of the “Low” class is 0 for all teams, which means that the winners have average to high performance. Some teams have higher “average” values than “high” indicating that they have chance to win even though their performance is not highly rated.

The histograms show the distributions of the wins and losses in all the winning World Series of all teams. Based on the height of the columns, we can recognize the performance that the teams usually have when they win a World Series tournament. It is hard to see the trend in the histograms of teams winning only once or twice, but teams like New York Yankee won many time and they have the number of wins around 95 to 105 with a peak at almost 98 and the number of losses between 50 to 65 with a peak at nearly 53 and 56.

------------------------------------------------------------------------

## Step 8: z-score table

```{r}
# Filter out rows where the WP column  or any of the relevant attributes are NA
data_filtered <- data %>%
  filter(!is.na(WP), !is.na(H), !is.na(SO), !is.na(SOA), !is.na(SHO), !is.na(FP))
```

For step 8, we will do the following steps:

Transform [H, SO, SOA, SHO, FP] into z-scores.

Fit a linear regression model to predict [Win Percentage] using the z-scored attributes.

Find the R² value and coefficients of the model.

```{r}
# Step 1: Transform the specified attributes into z-scores

data_z_processed <- data_filtered %>%
  mutate(
    H_z = scale(H),
    SO_z = scale(SO),
    SOA_z = scale(SOA),
    SHO_z = scale(SHO),
    FP_z = scale(FP)
  )
```

```{r}
# Step 2: Fit a linear model to predict WP using the z-scored attributes

model <- lm(WP ~ H_z + SO_z + SOA_z + SHO_z + FP_z, data = data_z_processed)

# Step 3: Report the R² value and the coefficients of the linear model
r_squared <- summary(model)$r.squared
coefficients <- summary(model)$coefficients
```

```{r}
# Print the R^2 value

cat("R² of the linear model:", r_squared, "\n\n")

# Print the coefficients of the model
cat("Coefficients of the linear model:\n")
print(coefficients)
```

### Interpretation:

|                        |                                                                                                                                                    |
|------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| **Intercept = 0.4957** | This represents the baseline success prediction when all other variables are 0. In this case, it is 49.57%.                                        |
| **H_z = 0.0197**       | This shows a slight positive relationship between Hits By Batters and team success. For every unit increase in H, success increases by 1.97%.      |
| **SO_z = -0.0767**     | This shows a negative relationship between Strikeouts By Batters and team success. For every unit increase in SO, success decreases by 7.67%.      |
| **SOA_z = 0.0734**     | This shows a positive relationship between Strikeouts By Pitchers and team success. For every unit increase in SOA, success increases by 7.34%.    |
| **SHO_z = 0.0302**     | This shows a positive relationship between Shutouts and team success. For every unit increase in SHO, success increases by 3.02%.                  |
| **FP_z = -0.0012**     | This shows a slight negative relationship between Fielding Percentage and team success. For every unit increase in FP, success decreases by 0.12%. |

------------------------------------------------------------------------

## Step 9: Three decision tree models

```{r}
# Filter the dataset for the relevant attributes (R to FP) and the TARGET column
data_filtered <- data %>%
  filter(!is.na(TARGET)) %>%
  select(R:FP, TARGET)

# Split the dataset into training and testing sets (80% training, 20% testing)
set.seed(42)  # For reproducibility
trainIndex <- createDataPartition(data_filtered$TARGET, p = 0.8, list = FALSE)
train_data <- data_filtered[trainIndex, ]
test_data <- data_filtered[-trainIndex, ]
```

Step 1: Build the first decision tree model with a max depth of 25 nodes

```{r}
model1 <- rpart(TARGET ~ ., data = train_data, method = "class", control = rpart.control(maxdepth = 5, cp = 0.01, maxcompete = 25))

# Visualize the first decision tree
rpart.plot(model1, main = "Decision Tree Model 1")
```

Step 2: Build the second decision tree model (try a different max depth or complexity parameter)

```{r}
model2 <- rpart(TARGET ~ ., data = train_data, method = "class", control = rpart.control(maxdepth = 4, cp = 0.02, maxcompete = 25))

# Visualize the second decision tree
rpart.plot(model2, main = "Decision Tree Model 2")
```

Step 3: Build the third decision tree model (with a different set of hyperparameters)

```{r}
model3 <- rpart(TARGET ~ ., data = train_data, method = "class", control = rpart.control(maxdepth = 3, cp = 0.015, maxcompete = 25))

# Visualize the third decision tree
rpart.plot(model3, main = "Decision Tree Model 3")
```

Step 4: Evaluate the accuracy of each model on both training and testing sets

```{r}
# calculate accuracy
calculate_accuracy <- function(model, train_data, test_data) {
  train_pred <- predict(model, newdata = train_data, type = "class")
  test_pred <- predict(model, newdata = test_data, type = "class")
  
  train_accuracy <- mean(train_pred == train_data$TARGET)
  test_accuracy <- mean(test_pred == test_data$TARGET)
  
  return(list(train_accuracy = train_accuracy, test_accuracy = test_accuracy))
}

accuracy_model1 <- calculate_accuracy(model1, train_data, test_data)
cat("Model 1 - Training Accuracy:", accuracy_model1$train_accuracy, "\n")
cat("Model 1 - Testing Accuracy:", accuracy_model1$test_accuracy, "\n\n")

accuracy_model2 <- calculate_accuracy(model2, train_data, test_data)
cat("Model 2 - Training Accuracy:", accuracy_model2$train_accuracy, "\n")
cat("Model 2 - Testing Accuracy:", accuracy_model2$test_accuracy, "\n\n")

accuracy_model3 <- calculate_accuracy(model3, train_data, test_data)
cat("Model 3 - Training Accuracy:", accuracy_model3$train_accuracy, "\n")
cat("Model 3 - Testing Accuracy:", accuracy_model3$test_accuracy, "\n\n")
```

### Interpretation:

Model Performance Overview:

-   Model 1 has the highest training and testing accuracy (90.81% training, 89.46% testing). This model (likely) fits the data better than the other two models.

-   2 and 3 have slightly worse accuracy both in training and testing, with Model 3 having the lowest performance (87.44% training, 85.01% testing).

Training vs Testing Accuracy:

-   Across all models, training accuracy is higher than testing accuracy, but the difference between them is small. This shows that none of the models are severely overfitting, as overfitting would manifest in a much larger difference between training and testing accuracies.

-   Model 1 has a slightly larger gap between training and testing accuracies compared to the others, which could suggest it is a bit more prone to overfitting than the other two models, but it's still within an acceptable range.

Based on the generated decision trees, the most important variable for determining the target variable is R (runs). Each tree places runs at the top (closest to the root), and then re-uses runs for fine-tuning the response. Other valuable variables are SV and RA, which are used in the second or third levels. All three trees are able to make their predictions from knowing just these three varaibles.

------------------------------------------------------------------------

## Step 10: Conclusion

placeholder text, put conclusion here. At most 13 sentences.
